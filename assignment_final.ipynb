{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "535c21960d4663d5edac398cb445d087",
     "grade": false,
     "grade_id": "jupyter",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "For this problem set, we'll be using the Jupyter notebook:\n",
    "\n",
    "![](jupyter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Datascience for Economics Final Assignment\n",
    "\n",
    "\n",
    "|Name|SNR|ANR|\n",
    "|----|---|----|\n",
    "|Luka Parisi|2066677|u500977|\n",
    "|Atma Jyoti Mahapatra|2084556|uxxxxxx|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research question "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9a9d08ea38644e3ac0b85955731a7e3",
     "grade": true,
     "grade_id": "cell-44d71caea99ce92b",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**This study will try to determine the relationship between poverty and crime in the US using county level data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Motivation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a74e1c61d4c263e5dcedc9c4f7565432",
     "grade": true,
     "grade_id": "cell-a00025e68181b6f6",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "There have been a multitude of different studies which have tried to disentangle the determinants and casual mechanisms behind crime. Yet, the exact mechanism by which crime functions is still quite ambiguous and is highly affected by the individual setting and a number of different factors. The economic theory of crime mostly focuses on factors such as poverty, education, family background, personal incentives, probability of arrest, race, sex, population density and other economic and social factors. Moreover, it is a key social concern to mitigate crime, and this thought is reflected by Aristotle, who succintly captured this by saying:\n",
    "\n",
    " ***\"Poverty is the parent of crime and revolution\"***\n",
    "\n",
    "\n",
    "Since we are both economists that are interested in poverty and inequality, we have choosen to analyze how poverty affects crime rates in the United States using county level data which was available for years 2010 and 2016. We explicitly choose to analyze data from the United States since it has historically been troubled with poverty levels that are quite high for a developed and rich country (source: [OECD](https://data.oecd.org/inequality/poverty-rate.htm)). Furthermore, the United States has a number of cities that have gained international notoriety for high levels of crime (such as Baltimore, St Louis, Detroit, and others.) and the country itself has been plagued with unexplainable instances of violent crime making it interesting for research.     \n",
    "\n",
    "Our initial idea was to analyze poverty and crime through the lense of each state individually. Instead, we decided it would be smarter to conduct our analysis on the county level since states can vastly differ by their respective counties. Hence, we concluded that looking at aggregated state data would not be as informative as county level data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method and data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58b91e9ed0432af5f7bd6e33a6a51515",
     "grade": true,
     "grade_id": "cell-a2a13b6c938ec4c8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "For this analysis we decided to merge three datasets:\n",
    "1. County-Level Detailed Arrest and Offense Data, United States, 2010 (https://www.icpsr.umich.edu/web/NACJD/studies/33523#): \n",
    "    - This data collection contains county-level counts of arrests and offenses for most counties in the United States for the year 2010.\n",
    "2. County-Level Detailed Arrest and Offense Data, United States, 2016 (https://www.icpsr.umich.edu/web/NACJD/studies/37059#):\n",
    "    - This data collection contains county-level counts of arrests and offenses for most counties in the United States for the year 2016. *We will explain the weaknesses of these two crime datasets in further detail below.*\n",
    "3. County level economic and demographic data for the years 2010 and 2016 (https://www.openintro.org/data/?data=county_complete):\n",
    "    - This was one of a few datasets where a vast amount of information was available for each county in the United States. It was compiled from a multitude of different sources such as the Census Bureau, Bureau of Labor Statistics, etc.\n",
    "\n",
    "*For full transparency datasets with all accompanying folders have been provided in our github repository.*\n",
    "\n",
    "The county level economic and demographic dataset for years 2010 and 2016 had a vast number of different variables (182). For our analysis we only needed a certain number of variables which were present for both the year of 2010 and 2016. Variables included:\n",
    "- name of state\n",
    "- name of county\n",
    "- fips code (this is our ID variable)\n",
    "- population (variable needed to calculate arrest rate per 100k and population density)\n",
    "- % of population that has obtained high school degree\n",
    "- % of population below poverty level (our independent variable of interest)\n",
    "- % of population unemployed\n",
    "- median household income\n",
    "- area of county (variable needed to calculate population density)\n",
    "    \n",
    "In the county level arrest datasets, we only choose the total number of arrests within the county in a given year (2010 and 2016, we will transform this variable into our dependent variable of interest) and the county and state code (which will be our ID variable and help us merge the two datasets) since all other demographic and economic variables were available in the previously mentioned county level dataset. \n",
    "\n",
    "The first challenge, beyond just choosing variables of interest, was to merge the multiple datasets. Each county in the United States has a unique FIPS code which we used as an identifier. Unfortunately, the datasets had different ways of representing this code. In order to merge the three datasets, we created a new column in the two FBI datasets in which we imputed the correct FIPS codes so that the FIPS codes would match between all of the datasets (we also modified the FIPS codes in the county-level dataset since they were missing a 0 for all the counties from the first 10 states). \n",
    "\n",
    "Furthermore, the county-level dataset grouped all variables of interest for both years (2010 and 2016) in columns further complicating the cleaning and merging. We decided that it would be easiest to compile two dataframes, county-level data merged with arrest statistics for the year 2010 and the same dataframe for the year of 2016. Later, we concatenated these two dataframes since they had identical columns, sorted them and set the respective indexes. Finally, we created a few variables such as arrest rate per 100k and population density through calculation of already available variables and finished cleaning the data.  \n",
    "\n",
    "After merging the data, we proceeded to deal with missing values and outliers (IQR). Through graphical exploration of our clean data, we could see that there was some form a linear relationship between our independent variable of interest - `poverty rate` and our dependent variable of interest - `arrest rate per 100k`. It suggested that higher poverty rates were correlated with higher rates of arrests. It is important to note that our dependent variable `arrest rate per 100k` was used as a proxy for crime and from now on we will refer to this variable as `crime per 100k`. In the next step we ran a linear regression using an ordinary least squared method. We later compared it with the results obtained by a Bayesian regression model. Before running our bayesian regression, we standardized all of our variables of interest as to help the algorithm. To wrap up our work, we evaluated the obtained results by interpreting the posterior distributions and traceplots and concluded on our findings. \n",
    "\n",
    "We specified the following model to test our hypothesis:\n",
    "\\begin{equation}\n",
    "\\label{eq:1}\n",
    "Crime\\ per\\ 100k = \\beta_0 + \\beta_1 Poverty\\ rate + \\beta_2 \\ln(Population\\ density)+ \\beta_3 Unemployment\\ rate\n",
    "\\end{equation}\n",
    "\n",
    "As noted in the introduction, besides the `poverty rate`, there are a multitude of factors that have been identified as determinants of crime. We choose to add population density as a control, since it has been cited that in areas with higher population density crime increases. We used the log of this variable since it had extreme values and was quite skewed. Furthermore, we also added the unemployment rate, hypothesizing that higher unemployment rates would increase arrest rates in counties. Previous graphical exploration of our control variables could show that the relationships were in the direction of our intuiton. We also wanted to include variables such as education and race, but they were highly correlated with other independent variables such as the `poverty rate`. Hence, we choose to exclude these variables, since multicollinearity would make our results harder to interpret.  \n",
    "\n",
    "We believe that our data is representative to the underlying population since it contained data from a large number of counties. Obviously, data collection could have been more frequent which would have enabled us to pinpoint changes in our key variables with more precision. Also, the crime data was collected by a number of different agencies in different counties making this process challenging and prone to error (as noted in the dataset documentation). In the  `Main assumptions` chapter we will further clarify certain problems with our approach that could have been solved if more variables could have been available. Furthermore, we will also explain the exact problems of the crime datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview of the answers (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a675684538158f19ff2a70fa98e7fff2",
     "grade": true,
     "grade_id": "cell-c2854f0b8b034fae",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "In line with our hypothesis, we find that there is a positive correlation between poverty and crime. By running a Bayesian analysis, we find the following key results:\n",
    "\n",
    "*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main assumptions (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d43d4f76461858e4d6140ccbf5347ab6",
     "grade": true,
     "grade_id": "cell-03cb6a4826b945c9",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Unfortunately we cannot interpret our findings as causal, since we believe there was a confounded relationship between poverty and crime rates. We were unable to control for probability of arrest/number of police within counties. This means that our results are probably biased and that the effect of poverty is smaller/larger than in the \"real world\". Furthermore, a high level of correlation between independent variables such as black/high school education/etc., with poverty made it even more demanding to specify our empirical specification such that we account for all control variables of interest. \n",
    "\n",
    "Knowing this beforehand, we obviously wanted to approach this problem through a methodology where we could isolate exogenous variation in poverty rates. Unfortunately for us (not so much for people that would be in the \"treatment\" group), poverty is not randomly assigned by birth and any experiment in this field would be deemed highly unethical meaning that data that would help us analyze this problem casually was lacking. Finally, we could not find a valid instrument since there is not enough variables at the county-level which would be highly correlated with poverty rates and not affect crime rates directly.\n",
    "\n",
    "To complete our already long list of problems, the crime datasets were missing all observations for two states: Florida and Illinois. Also, this data was collected individually by a large number of different agencies within different states and counties making it prone to errors. However, it was harmonized by the FBI, which allows us to place some degree of trust in the dataset.\n",
    "\n",
    "One might ask why we decided to work on this topic when we encountered all of these problems. The main reason was that we found this topic extraordinarly interesting and that even with these problems we believe that our findings still have value, even if they cannot be casaully interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python/R code (6 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up this shit\n",
    "\n",
    "#!pip install missingno\n",
    "import missingno as msno\n",
    "\n",
    "# Importing required modules\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# Bayesian\n",
    "import arviz as az\n",
    "#from chart_studio import plotly\n",
    "import plotly\n",
    "\n",
    "#!pip install plotly\n",
    "#!pip install fancyimpute\n",
    "#!pip install chart-studio\n",
    "\n",
    "#!pip install plotly-geo\n",
    "\n",
    "# preparing imputation\n",
    "\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading, merging and cleaning datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading county level data \n",
    "county = pd.read_csv('./Data/County_data/county_complete(1).csv')\n",
    "county.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting county level data for 2010 and choosing variables mentioned in the introduction; adding a year column for each dataset\n",
    "county2010 = county.loc[:,['state','name','fips',\n",
    "                           'pop2010','hs_grad_2010','poverty_2010','unemployment_rate_2010','median_household_income_2010','area_2010']]\n",
    "county2010['year'] = 2010\n",
    "# -||- for 2016\n",
    "county2016 = county.loc[:,['state','name','fips',\n",
    "                           'pop2016','hs_grad_2016','poverty_2016','unemployment_rate_2016','median_household_income_2016','area_2010']]\n",
    "county2016['year'] = 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading crime data for 2010\n",
    "crime2010 = pd.read_stata('./Data/FBI_crime_data/ICPSR_33523/DS0001/33523-0001-Data.dta')\n",
    "crime2010.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting crime data for 2010\n",
    "crime2010 = crime2010.loc[:,['FIPS_ST','FIPS_CTY','COVIND','GRNDTOT']]\n",
    "#Using str.zfill to make FIPS code mergable with county dataset(2010)\n",
    "crime2010['FIPS_ST'] = crime2010['FIPS_ST'].astype(str)\n",
    "crime2010['FIPS_ST'] = crime2010['FIPS_ST'].str.zfill(2)\n",
    "crime2010['FIPS_CTY'] = crime2010['FIPS_CTY'].astype(str)\n",
    "crime2010['FIPS_CTY'] = crime2010['FIPS_CTY'].str.zfill(3)\n",
    "crime2010[\"fips\"] = crime2010[\"FIPS_ST\"] + crime2010[\"FIPS_CTY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading crime data for 2016\n",
    "crime2016 = pd.read_stata('./Data/FBI_crime_data/ICPSR_37059/DS0001/37059-0001-Data.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting crime data for 2016 and using str.zfill to make FIPS code mergable with county dataset (2016)\n",
    "crime2016 = crime2016.loc[:,['FIPS_ST','FIPS_CTY','COVIND','GRNDTOT']]\n",
    "crime2016['FIPS_ST'] = crime2016['FIPS_ST'].astype(str)\n",
    "crime2016['FIPS_ST'] = crime2016['FIPS_ST'].str.zfill(2)\n",
    "crime2016['FIPS_CTY'] = crime2016['FIPS_CTY'].astype(str)\n",
    "crime2016['FIPS_CTY'] = crime2016['FIPS_CTY'].str.zfill(3)\n",
    "crime2016[\"fips\"] = crime2016[\"FIPS_ST\"] + crime2016[\"FIPS_CTY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finishing up cleaning fips codes in the county datasets such that the fips codes match for each county between all the datasets\n",
    "county2010['fips'] = county2010['fips'].astype(str)\n",
    "county2010['fips'] = county2010['fips'].str.zfill(5)\n",
    "county2016['fips'] = county2016['fips'].astype(str)\n",
    "county2016['fips'] = county2016['fips'].str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging dataframes for years 2010 and 2016 using .merge\n",
    "county_crime2010 = county2010.merge(crime2010,how='left',left_on=['fips'], right_on=['fips'])\n",
    "county_crime2016 = county2016.merge(crime2016,how='left',left_on=['fips'], right_on=['fips'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming columns such that they have the same names and can be merged together using pd.concat\n",
    "county_crime2010.rename({'pop2010':'population','hs_grad_2010':'hs_grad','poverty_2010':'poverty', 'unemployment_rate_2010':'unemployment_rate',\n",
    "                          'median_household_income_2010':'median_hh_income'},inplace=True,axis=1)\n",
    "county_crime2016.rename({'pop2016':'population','hs_grad_2016':'hs_grad','poverty_2016':'poverty', 'unemployment_rate_2016':'unemployment_rate',\n",
    "                          'median_household_income_2016':'median_hh_income'},inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking result of merge\n",
    "county_crime2010.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -||-\n",
    "county_crime2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging dataframes for 2010 and 2016 and dropping columns that we dont need anymore; finally checking result of concat\n",
    "frames = [county_crime2010,county_crime2016]\n",
    "county_crime = pd.concat(frames)\n",
    "county_crime = county_crime.reset_index()\n",
    "county_crime.drop(['FIPS_ST','FIPS_CTY','index'],inplace = True,axis=1)\n",
    "county_crime.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating variables of interest that are needed for further analysis:\n",
    "Arrest rate – An arrest rate describes the number of arrests made by law enforcement agencies per \n",
    "100,000 total population. We calculate arrest rate by dividing the number of reported arrests by the county population. The result is multiplied by 100,000. We will use this variable as our dependent variable and proxy variable for crime rate per 100k.\n",
    "\n",
    "Natural logarithm of population density - Population density = County population / Land area. After this we take the natural logorithm of population density since the variable has extreme values and it will be easier to work in the logorithmic form. Also the interpretation of the variable will be easier since we can see how a 1% change in population density can affect crime rates.\n",
    "\n",
    "Furthermore, we create a variable which we will use in the next chapter to create a US county map for arrest rates. We will delete this variable once we are done with the introdoctury graphical exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculations  \n",
    "county_crime['arrest_rate'] =  (county_crime.GRNDTOT/county_crime.population)*100000\n",
    "county_crime['l_population_density'] = np.log(county_crime.population/county_crime.area_2010)\n",
    "county_crime['arrest_rate_map'] = (county_crime.GRNDTOT/county_crime.population)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introductory graphical exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#american map - figure\n",
    "fig = px.choropleth(county_crime,\n",
    "                    geojson = counties,\n",
    "                    locations='fips',\n",
    "                    color='arrest_rate_map',\n",
    "                    color_continuous_scale=\"Viridis\",\n",
    "                    range_color=(0, 5),\n",
    "                    scope=\"usa\",\n",
    "                    animation_frame=\"year\",\n",
    "                    labels={'arrest_rate_map':'Arrest rate'},\n",
    "                    width = 800,\n",
    "                    height = 600,\n",
    "                    title = ' Figure 1 - Change in arrest rate in US counties'\n",
    "                    )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this map shows, arrest rates have more or less stayed around the same levels between the two years, with any slight changes being reflected in a downward direction. Furthermore, through graphical exploration we can see that a large number of counties have an arrest rate of 0. The zeroes are clustered around the states of Illinois and Florida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataset for graphical exploration; checking distribtuion of variables\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "sns.histplot(data=county_crime, x=\"arrest_rate\", kde=True, color=\"skyblue\", ax=axs[0, 0])\n",
    "sns.histplot(data=county_crime, x=\"poverty\", kde=True, color=\"olive\", ax=axs[0, 1])\n",
    "sns.histplot(data=county_crime, x=\"hs_grad\", kde=True, color=\"gold\", ax=axs[1, 0])\n",
    "sns.histplot(data=county_crime, x=\"unemployment_rate\", kde=True, color=\"teal\", ax=axs[1, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at distribution of dependent variable through 2010 and 2016\n",
    "fig = px.histogram(county_crime, x=\"arrest_rate\", color=\"year\", width = 500, height = 400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the distribtuion of our variable of interest - arrest rate per 100k, there are a large number of zeroes in our dataset and a few large outliers from 10.000 up to 60.000. We will deal with this problem in the following chapters. This large number of  observed zeroes, as we will note in the next chapter, is a fault of this dataset which contains true zeroes and zeroes that constitute missing data (instead of NaN's the original dataset has inputed 0 for missing observations).\n",
    "\n",
    "Moreover, the variable of interest, `arrest_rate`, is distributed in a similar manner between 2010 and 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(county_crime, width = 500, height = 400, \n",
    "                   x=\"poverty\", \n",
    "                   color=\"year\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our independent variable of interest `poverty` seems to have a more normal-like distribution. Most of the values are centered around 10% to 20%, with a few small and large values. This variable is also distributed roughly similarly between the two years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final sorting of dataset and setting indexes for next chapters so that we can deal with outliers and missing values; checking final result\n",
    "county_crime.drop(['area_2010','arrest_rate_map'],inplace=True,axis=1)\n",
    "county_crime = county_crime.sort_values(['fips','year'])\n",
    "county_crime.set_index(['fips','name','year'],inplace=True)\n",
    "county_crime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Defining a function which can analyze the dataset in terms of missing values and type of columns\n",
    "def Analysis(data):\n",
    "    print(\"Analysing\")\n",
    "    print(data.info())\n",
    "    #Calculating the share of missing values\n",
    "    missing = pd.concat([data.isnull().sum(), 100 * data.isnull().mean()], axis=1)\n",
    "    missing.columns=['Count', '%']\n",
    "    missing = missing.sort_values(by='Count', ascending=False)\n",
    "    print(missing)\n",
    "    df=missing.iloc[0:5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Analysis(county_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our \"final\" dataset has only a small amount of missing values. But the bigger problem lies within the true and false zeroes, as we saw during graphical exploration. As noted in the data documentation \"In this data collection, zeroes may represent true zeroes or missing data, and it is possible to distinguish between the two\". The following image shows the difference between a true zero and a zero that represents missing data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Methodology for missing data](Missing_data.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, zeroes that are not true zeroes have a coverage indicator of 0 and all arest count variables that are equal to 0 (basically our key variable equal to 0, because it is built on the sum of all arrest count variables from the original dataset). To find these true zeroes, we will use the code in the following cell, and transform the arrest rate to NaN if it follows the condition from the above attached image (Coverage indicator = 0 AND Arrest rate = 0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = ['arrest_rate', 'COVIND', 'GRNDTOT']\n",
    "county_crime[cols] = county_crime[cols].mask(county_crime[cols].eq(0).all(axis=1))\n",
    "#Drop all variables we dont need anymore\n",
    "county_crime.drop(['COVIND','GRNDTOT'],inplace=True,axis=1)\n",
    "msno.matrix(county_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the missing observation matrix, most of the missing values are connected meaning that they come from the same states - Florida and Illinois. In the next fragment of code we will see the exact number of missing observations and we will discuss our way of dealing with this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Analysis(county_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a total of 362 missing values for arrest rates (5.76% of our dataset). In our work, we will try two approaches for dealing with missing data:\n",
    "1. We will drop the missing variables (complete case analysis - because only records that are complete get retained for the analysis)\n",
    "2. We will use IterativeImputer or popularly called MICE (Multiple Imputation by Chained Equation) for imputing missing values \n",
    "\n",
    "The IterativeImputer performs multiple regressions on random samples of the data and aggregates for imputing the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1. Dropping all missing values \n",
    "county_crime_dropna = county_crime.dropna()\n",
    "county_crime_dropna.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2. MICE imputation\n",
    "# Initializing the imputer\n",
    "MICE_imputer = IterativeImputer(skip_complete=True)\n",
    "county_crime_mice = county_crime.iloc[:,1:]\n",
    "# Imputing the missing observations\n",
    "county_crime_mice.iloc[:, :] = MICE_imputer.fit_transform(county_crime_mice)\n",
    "county_crime_mice.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimating linear regression\n",
    "X = sm.add_constant(county_crime_dropna[['poverty','l_population_density','unemployment_rate']])\n",
    "y = county_crime_dropna['arrest_rate']\n",
    "lm = sm.OLS(y, X).fit()\n",
    "print('\\nSummary drop na: ', lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = sm.add_constant(county_crime_mice[['poverty','l_population_density','unemployment_rate']])\n",
    "y = county_crime_mice['arrest_rate']\n",
    "lm = sm.OLS(y, X).fit()\n",
    "print('\\nSummary mice: ', lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model estimated for MICE dataset has an R squared that is slighly higher than the dataset in which we dropped all missing values (i.e. 0.3% more variation in y is explained by our x values in our MICE dataset). Even though the arrest rate values were imputed, we are still using information from all of our independent variables (363 more obs). Taking this into account we will continue working with the mice dataset in the further analysis. One thing to note is that our r-squared is rather low. One of the factors that may affect this are large outliers in our dependent variable - arrest rate. In the next chapter we will deal with these outliers in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detecting outliers in our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter we plotted the distribution of arrest rate and we saw that it has extreme values in the right tail. Furthermore, we can see that the max arrest rate and standard deviation for arrest rate is quite high. Since our goal is to do regression analysis, we will exclude some of the largest outliers such that our results dont reflect results that are solely created by these large outlier values and so that our model can have a better fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring highest values of arrest rate per 100k\n",
    "print(county_crime_mice['arrest_rate'].nlargest(n=10))\n",
    "print(county_crime_mice['arrest_rate'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrest rates as high as these would mean that 63.221 out of 100.000 people have been arrested within a given county. These are extreme values which probably do not accurately reflect the crime in a given county. To detect and clean outliers from our data we will use the interquartile range (IQR). IQR is a measure of statistical dispersion and is calculated as the difference between the 75th and 25th percentiles. It is represented by the formula IQR = Q3 − Q1. The lines of code below create a function that calculates and print the interquartile range for our final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers = []\n",
    "def detect_outliers_iqr(data):\n",
    "    data = sorted(data)\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    # print(q1, q3)\n",
    "    IQR = q3-q1\n",
    "    lwr_bound = q1-(1.5*IQR)\n",
    "    upr_bound = q3+(1.5*IQR)\n",
    "    # print(lwr_bound, upr_bound)\n",
    "    for i in data: \n",
    "        if (i<lwr_bound or i>upr_bound):\n",
    "            outliers.append(i)\n",
    "    return outliers# Driver code\n",
    "sample_outliers_mice = detect_outliers_iqr(county_crime_mice['arrest_rate'])\n",
    "print(sample_outliers_mice)\n",
    "print(len(sample_outliers_mice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataset has 184 outliers. All of the outliers are on the right side of the distribution. Hence, we will remove all of these outliers in the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop outliers, we are using min(sample_outliers) because we only have outliers on the right of our distribution\n",
    "county_crime_mice = county_crime_mice.loc[county_crime_mice['arrest_rate'] < min(sample_outliers_mice)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "sns.histplot(data=county_crime_mice, x=\"arrest_rate\", kde=True, color=\"skyblue\", ax=axs[0, 0]).set(title='Distribution of arrest rate (MICE Imputation)')\n",
    "sns.histplot(data=county_crime_mice, x=\"poverty\", kde=True, color=\"olive\", ax=axs[0, 1]).set(title='Distribution of poverty rate (MICE Imputation)')\n",
    "sns.histplot(data=county_crime, x=\"arrest_rate\", kde=True, color=\"skyblue\", ax=axs[1, 0]).set(title='Distribution of arrest rate (Baseline)')\n",
    "sns.histplot(data=county_crime, x=\"poverty\", kde=True, color=\"olive\", ax=axs[1, 1]).set(title='Distribution of poverty rate (Baseline)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(county_crime_mice['arrest_rate'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show that we have successfuly dealt with the outliers in the data, and the data is not as skewed as it was in the original dataset. The variable of interest `arrest_rate` has a much less skewed distribution, while other variables such as `poverty` are not affected by this method. Finally we check the skew index of `arrest_rate` and we can see that it reduces drastically from 4.86 to 0.37, indicating that the method effectively removed outliers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationship between `arrest_rate` and other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we lay our foundations for how we expect `arrest_rate` to move, with other variables. This is based on economic theory, and then we present graphs based on the data, that test our hypothesis. \n",
    "\n",
    "* We can expect `arrest_rate` to **increase** as `poverty` increases. If people are under the poverty level, they turn to crime to try to sustain their lives and consequentially, arrest rates are higher.\n",
    "\n",
    "* We can expect `arrest_rate` to **increase** as `unemplyment_rate` increases. Similar to `poverty`, `unemployment` also can lead people to a criminal lifestyle. However, the important difference is that unemployed people are usually better educated and by definition, are still part of the labor force. They have an incentive to rejoin the labor force, and earn a sustenance.\n",
    "\n",
    "* We can expect `arrest_rate` to **decrease** as `hs_grad` increases. As more people finish high school and get an education, they move into the labor force and usually stay out of crimes for which they can be arrest.\n",
    "\n",
    "* We can expect `arrest_rate` to **increase** as `L_population_density` increases. This is more of an intuitive understanding, where an increase in population opens up more opportunities for criminals to commit crime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "sns.regplot(x=county_crime_mice['poverty'], y=county_crime_mice['arrest_rate'], ax=axs[0, 0]).set(title='Poverty and arrest rate')\n",
    "sns.regplot(x=county_crime_mice['unemployment_rate'], y=county_crime_mice['arrest_rate'], ax=axs[0, 1]).set(title='Unemployment and arrest rate')\n",
    "sns.regplot(x=county_crime_mice['hs_grad'], y=county_crime_mice['arrest_rate'], ax=axs[1, 0]).set(title='Education and arrest rate')\n",
    "sns.regplot(x=county_crime_mice['l_population_density'], y=county_crime_mice['arrest_rate'], ax=axs[1, 1]).set(title='Population density and arrest rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the three plots above, even though most of the observations are stacked together all of the relationships are as we anticipated. With an increase in poverty rates/population density/unemployment rate, it seems like arrest rates also increase slightly, and the opposite holds true for education and arrest rates.\n",
    "\n",
    "Having laid down the groundwork for the intuitive reasoning behind our dependent and independent variables, and having cross-checked and validated this pattern in the data, we now move on to the analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction a correlation Heatmap\n",
    "corrMatrix = county_crime_mice.corr()\n",
    "#makes a correlation matrix using the relevant variables\n",
    "fig8, ax = plt.subplots(figsize=(9,7.5))\n",
    "ax.set_title('Correlation Matrix',fontweight=\"bold\")\n",
    "sns.heatmap(corrMatrix, annot=True, linewidths=0.75,cmap=\"YlGnBu\");\n",
    "# sns.heatmap plots a correlation heatmap\n",
    "# annot shows the corrleation value\n",
    "# line width just to have some space between squares\n",
    "# cmap to choose a different colour scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our independent variable poverty is highly correlated with hs_grad (% of population that has obtained high school degree) and median_hh_income (median household income). Unfortunately, because of this we will exclude hs_grad from the analysis even though we still believe that education is an important determinant of crime. We will now specify our regression model:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:2}\n",
    "Crime\\ per\\ 100k = \\beta_0 + \\beta_1 Poverty\\ rate + \\beta_2 \\ln(Population\\ density)+ \\beta_3 Unemployment\\ rate\n",
    "\\end{equation}\n",
    "\n",
    "Interestingly, poverty and unemployment do not seem to have a very high correlation. The official definitions are:\n",
    "\n",
    "* **Unemployed**: People who are jobless, looking for a job, and available for work are unemployed.\n",
    "\n",
    "* **Poverty**: If a family's total income is less than the family's threshold, then that family and every individual in it is considered in poverty.\n",
    "\n",
    "This implies that while people in poverty can work and still earn less than the threshold, unemployed people by definition do not work, and are actively seeking work. While it may be difficult for the former group to exit the *poverty trap*, it is also important to account for the latter, so that we can understand how economic fluctuations can also affect crime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying regression\n",
    "X = sm.add_constant(county_crime_mice[['poverty','l_population_density','unemployment_rate']])\n",
    "y = county_crime_mice['arrest_rate']\n",
    "lm = sm.OLS(y, X).fit()\n",
    "print('\\nSummary_mice: ', lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequentist conclusion from our regression output would be that given our data, a 1% increase in poverty rate would increase the number of arrests per 100k by 42. Our control variables also have positive coefficients as hypothesized, meaning with a 1 unit increase in population density/unemployment rate, arrest rates would also increase. All of the variables are statstically significant given p-values of 0.00 (besides the constant, but the constant in this specification is not relevant). Obviously, our large sample of 6100 observations would probably mean that inserting any of our even remotely related variables would yield statistically significant findings (if variables would not be highly correlated between each other). Our R-squared is 0.109, meaning that about 11% of the overall variation in our dependent variable is explained by our independent variables. Even though R-squared is not very important for causal inference and economic models, it still feels like there is a lot of information we are missing when trying to explain crime rates. Our next step will be to run our bayesian regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help the alogirthm, we will first standardize our variables of interest. We have used the same standardization method as in the notebook -\n",
    "\n",
    "Standardization:\n",
    "\\begin{equation}\n",
    "\\label{eq:3}\n",
    "x_{standardized} = (x - mean(x)) / std(x)\n",
    "\\end{equation} \n",
    "\n",
    "After standardization we will complete the specification of our bayesian model. As in the notebook, we choose a small value for our $\\sigma_{prior}$, all in the purpose of avoiding overfitting. We sample 1500 posterior draws, while 750 burn-in draws are sampled and discarded. We have 5 parameters - the intercept, poverty rate, $\\ln$ of population density, unemployment rate and the standard deviation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    return (x-x.mean())/x.std()\n",
    "arrest_rate_norm = standardize(county_crime_mice['arrest_rate'])\n",
    "poverty_norm = standardize(county_crime_mice['poverty'])\n",
    "l_population_density_norm = standardize(county_crime_mice['l_population_density'])\n",
    "unemployment_rate_norm = standardize(county_crime_mice['unemployment_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pm.Model() as normal:\n",
    "    constant = pm.Normal('constant', mu = 0.0, sd = 1.0)\n",
    "    σ_prior = 0.1\n",
    "    b_poverty = pm.Normal('b_poverty', mu = 0, sd = σ_prior)\n",
    "    b_l_population_density = pm.Normal('b_l_population_density', mu = 0, sd = σ_prior)\n",
    "    b_unemployment_rate = pm.Normal('b_unemployment_rate', mu = 0, sd = σ_prior)\n",
    "    \n",
    "    μ = constant + b_poverty*poverty_norm + b_l_population_density*l_population_density_norm + b_unemployment_rate*unemployment_rate_norm\n",
    "    σ = pm.HalfNormal('σ', 1)\n",
    "\n",
    "    arrest_rate = pm.Normal('arrest_rate', μ, σ, observed=arrest_rate_norm)\n",
    "    trace_normal = pm.sample(1500, tune=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with normal:\n",
    "    ppc_normal = pm.sample_posterior_predictive(trace_normal, var_names = ['arrest_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_posterior_normal = az.from_pymc3(trace_normal, posterior_predictive = ppc_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variables_arrest = ['b_poverty','b_l_population_density','b_unemployment_rate']\n",
    "az.summary(data_posterior_normal.posterior,variables_arrest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R hat values have a value of 1, which confirms that all the chains converged succesfuly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with normal:\n",
    "    pm.plot_trace(trace_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Results from the trace plots\n",
    "\n",
    "When looking at the distributions, we can see that each posterior distribution is centered around a certain value: \n",
    "\n",
    "* The `b_poverty_rate` is around 0.16 \n",
    "* The `b_l_population_density` is around 0.23\n",
    "* The `b_unemployment_rate` is around 0.13\n",
    "\n",
    "There are three features to look at when observing the trace plots:\n",
    "1. The plot should be stationary - not trending upwards or downwards\n",
    "2. Condensed zig-zagging of the trace \n",
    "3. Different chains covering same regions i.e. convergance\n",
    "\n",
    "In our trace plots all three features are satisfied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'y':arrest_rate_norm, 'x1':poverty_norm, 'x2':l_population_density_norm, 'x3':unemployment_rate_norm })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model fit plot\n",
    "percentiles = np.percentile(ppc_normal['arrest_rate'],[2.5,97.5],axis=[0]).T\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(30, 15))\n",
    "\n",
    "ax1.vlines(df.x1,percentiles[:,0],percentiles[:,1],alpha=0.09)\n",
    "ax1.scatter(df.x1,df.y, color='r')\n",
    "ax1.set(xlabel='$poverty$', ylabel='$arrest_rate$');\n",
    "\n",
    "ax2.vlines(df.x2,percentiles[:,0],percentiles[:,1],alpha=0.09)\n",
    "ax2.scatter(df.x2,df.y, color='r')\n",
    "ax2.set(xlabel='$population_density$', ylabel='$arrest_rate$');\n",
    "\n",
    "ax3.vlines(df.x3,percentiles[:,0],percentiles[:,1],alpha=0.09)\n",
    "ax3.scatter(df.x3,df.y, color='r')\n",
    "ax3.set(xlabel='$unemployment_rate$', ylabel='$arrest_rate$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness analysis (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c76eccb6035c07aa27eec1492ed7ed3",
     "grade": true,
     "grade_id": "cell-365f4bd48b20ea77",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Even though we dealt with outliers in our dependent variables of interest, there are still a few outliers in our independent variables. These outliers could be driving the results of our findings. Furthermore, we believe our findings would be more robust if we could find a valid instrument for our independent variable of interest: `poverty rate`. Finally, as noted in the introduction, we are not making any grandoise claims of causality since we acknowledge that we didnt have a source of random exogenous variation in our independent variable of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and conclusion (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e908b508af5c2244449588615f83a4d",
     "grade": true,
     "grade_id": "cell-f8f3ab0fc4655b00",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "We find that unemployment rate, population density and poverty are correlated with crime rates, and move in the same direction. While the effect is anywhere between 0.13 to 0.23 (in their respective units, viz-a-viz the model), it is an important conclusion to understand which factors are related to poverty.\n",
    "\n",
    "As mentioned earlier, the dataset we use is at the county level, and over two time periods. So there is a lot of \"scatter\" in the data, which resullts in low $R^2$ for our models. Furthermore, we are missing data on police funding, and more specifically, the probability of arrest, which could give us more insightful results. Lastly, we would like to use time-series data dating back to multiple years, instead of just two years. These addendums to this project would result in more robust findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
